{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Procesing\n",
    "\n",
    "- Only using the data between 1st January and 25th December which should cover the year for me\n",
    "    - These are kept in UTC for now as my travels spanned across multiple timezones.\n",
    "    - Some activities had to be reviewed manually to set the start and end dates.\n",
    "    - Checks were done by comparing the activity coordinates manually on google to confirm what the best datetime to be used.\n",
    "    - Extra checks are added by checking the end time with start and end datetime for activity to avoid overlapping activities as defined in the next section.\n",
    "- Google Maps Timeline\n",
    "    - Data is divided into 4 types - Visits, Activities, Timeline Paths, and Timeline Memory\n",
    "    - **Visit**: Usually stores when you have been static on the map for a while, unsure how this works fully as it tends to capture highway stops but misses some restaurant visits. Connectivty could be a factor.\n",
    "    - **Activity**: While movement is detected, google tries to predict if you were driving, walking, cycling, etc. This provides info on strat and end coordinates along with distace - so speed is a factor used to determine activity type. Again bad connectivity could factor in for error.\n",
    "    - **Timeline Paths**: While activities only store the start and end coordinates along with their time, Timeline Paths store a dictionary of coordinates between those times to give a full picture of points visited usually captured at a frequency of few minutes.\n",
    "    - **Timeline Memory** - My data contained very little of these for me to make any concrete definition for them and they have not been used in the analysis.\n",
    "- Each row in JSON data can depict either of the above types and need a check to fill in an NA values to make their identification easier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# read the JSON file\n",
    "with open('data/Timeline_backup_20241224.json') as json_data:\n",
    "    data = json.load(json_data)\n",
    "    df_timelines = pd.json_normalize(data['semanticSegments'])\n",
    "\n",
    "# PRE_PROCESSING\n",
    "\n",
    "# replace any null values for columns being used\n",
    "df_timelines['timelinePath'] = df_timelines['timelinePath'].fillna(0)\n",
    "df_timelines['visit.probability'] = df_timelines['visit.probability'].fillna(0)\n",
    "df_timelines['activity.probability'] = df_timelines['activity.probability'].fillna(0)\n",
    "\n",
    "# convert to datetime format\n",
    "df_timelines['startTime'] = pd.to_datetime(df_timelines['startTime'], utc=True)\n",
    "df_timelines['endTime'] = pd.to_datetime(df_timelines['endTime'], utc=True)\n",
    "\n",
    "# filter only for the needed travel dates\n",
    "travel_start_date = '2024-01-01 16:00:00'\n",
    "travel_end_date = '2024-12-24 05:30:00'\n",
    "\n",
    "df_timelines = df_timelines[df_timelines['startTime'] >= pd.to_datetime(travel_start_date, utc=True)]\n",
    "df_timelines = df_timelines[df_timelines['startTime'] <= pd.to_datetime(travel_end_date, utc=True)]\n",
    "df_timelines = df_timelines[df_timelines['endTime'] <= pd.to_datetime(travel_end_date, utc=True)]\n",
    "\n",
    "# reset the index before finalized the data frame\n",
    "df_timelines = df_timelines.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis 1\n",
    "- Get the count of most visits over the year and remove \n",
    "- Get it by count of visits to a place on differetn date or time, as well as get it by the time spent at the location\n",
    "\n",
    "## Processing:\n",
    "- Only select the visit related columns and rows for now\n",
    "- Use a split funstion to get the visit lattitude and longitude\n",
    "- Aggregate to get the total visits by count as well as time spent there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "needed_cols = ['startTime','endTime','visit.hierarchyLevel','visit.probability','visit.topCandidate.placeId','visit.topCandidate.semanticType','visit.topCandidate.probability','visit.topCandidate.placeLocation.latLng']\n",
    "df_temp = df_timelines[df_timelines['visit.probability']!=0]\n",
    "df_temp = df_temp[needed_cols]\n",
    "\n",
    "df_temp['time_spent'] = df_temp['endTime']-df_temp['startTime']\n",
    "df_temp[['visit.lat','visit.long']] = df_temp['visit.topCandidate.placeLocation.latLng'].str.split(',',expand=True)\n",
    "df_temp['visit.lat'] = df_temp['visit.lat'].apply(lambda x: round(float(x[:-2]), 4))\n",
    "df_temp['visit.long'] = df_temp['visit.long'].apply(lambda x: round(float(x[:-2]), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "groupby_cols = ['visit.lat','visit.long']\n",
    "selected_cols = ['time_spent']\n",
    "df_temp = df_temp[df_temp['visit.topCandidate.semanticType']==\"UNKNOWN\"]\n",
    "df_tempGroup_count = df_temp[selected_cols + groupby_cols].groupby(groupby_cols).count().sort_values(by='time_spent',ascending=False).reset_index()\n",
    "df_tempGroup_timeSpent = df_temp[selected_cols + groupby_cols].groupby(groupby_cols).sum().sort_values(by='time_spent',ascending=False).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add some columns and export to csv so data can be reviewed over excel\n",
    "# merge the two dfs\n",
    "df_temp = df_tempGroup_count.merge(df_tempGroup_timeSpent,\"outer\",on=groupby_cols)\n",
    "\n",
    "# add columns\n",
    "df_temp[\"time_mins\"] = df_temp['time_spent_y'].astype('timedelta64[m]')\n",
    "df_temp[\"googleMap_link\"] = \"https://maps.google.com/?q=\" + df_temp[\"visit.lat\"].astype(str) + \",\" + df_temp[\"visit.long\"].astype(str)\n",
    "df_temp[\"visit_desc\"] = \"\"\n",
    "\n",
    "# save as csv\n",
    "df_temp.to_csv(\"data/analysis1_output.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis 2\n",
    "- Check how many times I visited a park\n",
    "- Park visits maynot be considered as a visit in Google Timelines data since walks are not considered as a visit\n",
    "- The activites data will have to be used to count park visits\n",
    "\n",
    "## Processing:\n",
    "- Make a dictionary of parks to be considered\n",
    "- Capture their cordinates as a list and then review all the activity data\n",
    "- Check if the given point exists in any of the park polygons created\n",
    "- Keep a count for each park and increase it as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point, Polygon\n",
    "\n",
    "# cordinates have been extracted and rounded to 4 decimals manually\n",
    "parks_dict = {\n",
    "    \"CP\" : Polygon([(40.7681, -73.9816),(40.7644, -73.9731),(40.7968, -73.9494),(40.8004, -73.9580)]),\n",
    "    \"WSP\" : Polygon([(40.7321, -73.9986),(40.7307, -73.9956),(40.7296, -73.9965),(40.7310, -73.9995)]),\n",
    "    \"Union\" : Polygon([(40.7352, -73.9916),(40.7370, -73.9903),(40.7365, -73.9892),(40.7349, -73.9904)]),\n",
    "    \"MSP\" : Polygon([(40.7433, -73.9881),(40.7427, -73.9867),(40.7410, -73.9879),(40.7414, -73.9889),(40.7422, -73.9888)]),\n",
    "    \"Bryant\" : Polygon([(40.7546, -73.9840),(40.7534, -73.9811),(40.7523, -73.9819),(40.7535, -73.9848)]),\n",
    "    # \"Gantry\" : Polygon([(40.7488, -73.9588),(40.7468, -73.9572),(40.7459, -73.9582),(40.7450, -73.9579),(40.7436, -73.9595),(40.7384, -73.9616),(40.7386, -73.9628)]),        # manual with some incorrect cordinates for specific activity points\n",
    "    \"Gantry\" : Polygon([(40.7387, -73.9601),(40.7385, -73.9614),(40.7395, -73.9619),(40.7401, -73.9611),(40.7414, -73.9607),(40.7423, -73.96),(40.7437, -73.9594),(40.7454, -73.9575),(40.7456, -73.9582),(40.7461, -73.9582),(40.7467, -73.9573),(40.7477, -73.9574),(40.7483, -73.9576),(40.7427, -73.9612),(40.7384, -73.9629),(40.7381, -73.9604),(40.7387, -73.9601)]),\n",
    "    \"QueensBridge\" : Polygon([(40.7547, -73.9490),(40.7554, -73.9506),(40.7581, -73.9484),(40.7572, -73.9466)]),\n",
    "    \"Astoria\" : Polygon([(40.7767, -73.9276),(40.7754, -73.9250),(40.7812, -73.9181),(40.7824, -73.9197)]),\n",
    "    \"Prospect\" : Polygon([(40.6729, -73.9698),(40.6631, -73.9628),(40.6549, -73.9621),(40.6513, -73.9719),(40.6583, -73.9742),(40.6610, -73.9796)]),\n",
    "    \"Greenwood\" : Polygon([(40.6594, -73.9951),(40.6590, -73.9883),(40.6551, -73.9820),(40.6478, -73.9805),(40.6443, -73.9890),(40.6529, -74.0019)])\n",
    "}\n",
    "\n",
    "# only consider the activites calcuated by the timelines\n",
    "df_temp = df_timelines[df_timelines['timelinePath']!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latlong_point(lat_long):\n",
    "    # split the string on the comma\n",
    "    split_latlong = lat_long.split(\",\")\n",
    "    # remove the degree signs to return lattitude and longitude\n",
    "    lat = round(float(split_latlong[0][:-2]),4)\n",
    "    lon = round(float(split_latlong[-1][:-2]),4)\n",
    "\n",
    "    return Point(lat, lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bryant 42\n",
      "Gantry 31\n",
      "CP 14\n",
      "WSP 13\n",
      "Prospect 6\n",
      "MSP 5\n",
      "QueensBridge 5\n",
      "Union 2\n",
      "Astoria 1\n",
      "Greenwood 1\n"
     ]
    }
   ],
   "source": [
    "park_count = {}\n",
    "park_lastDate = {}\n",
    "\n",
    "# adding all keys with default values to the dictionaries\n",
    "for x in parks_dict:\n",
    "    park_count[x] = 0\n",
    "    park_lastDate[x] = 0\n",
    "\n",
    "for i,x in df_temp.iterrows():\n",
    "    # go through each point\n",
    "    for p in x['timelinePath']:\n",
    "        temp_point = get_latlong_point(p['point'])\n",
    "        for park in parks_dict:\n",
    "            # check if the point is in any of the parks and on a different date as previous accounted visit    \n",
    "            if parks_dict[park].contains(temp_point) and x['startTime'].date() != park_lastDate[park]:\n",
    "                park_count[park] += 1\n",
    "                park_lastDate[park] = x['startTime'].date()\n",
    "                # if the point is found move to the next activity\n",
    "                break\n",
    "\n",
    "for park_info in sorted(park_count.items(), key=lambda x: (-x[1], x[0])):\n",
    "    print(park_info[0],park_info[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
